{
  "version": "1.0.0",
  "last_updated": "2025-11-11",
  "description": "Prompts for Live Conversational Threads AI analysis",
  "prompts": {
    "initial_clustering": {
      "description": "Generate initial topic-based nodes from transcript",
      "model": "gpt-4",
      "temperature": 0.5,
      "max_tokens": 4000,
      "template": "You are analyzing a conversation transcript to identify natural topic shifts and create a hierarchical graph structure.\n\nGiven the following conversation with {utterance_count} utterances from {participant_count} participants:\n\nParticipants: {participants}\n\nTranscript:\n{transcript}\n\nTask: Identify natural topic boundaries and create nodes at 5 different zoom levels:\n\n1. SENTENCE (Level 1): Individual important sentences or short exchanges\n2. TURN (Level 2): Speaker turns or complete thoughts\n3. TOPIC (Level 3): Distinct topics or sub-discussions (3-10 utterances)\n4. THEME (Level 4): Major themes or discussion areas (10-30 utterances)\n5. ARC (Level 5): Overall narrative arcs or meeting segments (30+ utterances)\n\nFor each node, provide:\n- title: Brief descriptive title (5-10 words)\n- summary: Concise summary of what was discussed\n- zoom_levels: Array of zoom levels where this node should be visible [1-5]\n- start_utterance: Index of first utterance (0-based)\n- end_utterance: Index of last utterance (0-based)\n- primary_speaker: Main speaker for this segment (if applicable)\n- keywords: 3-5 key terms or concepts\n\nReturn a JSON array of nodes. Ensure:\n- Good coverage across all 5 zoom levels\n- Nodes at higher zoom levels (4-5) encompass lower level nodes\n- Natural topic boundaries (don't split mid-thought)\n- Each utterance belongs to at least one node\n\nExample response:\n[\n  {{\n    \"title\": \"Opening and Introductions\",\n    \"summary\": \"Team members greet each other and Alice opens the meeting\",\n    \"zoom_levels\": [3, 4, 5],\n    \"start_utterance\": 0,\n    \"end_utterance\": 5,\n    \"primary_speaker\": \"Alice\",\n    \"keywords\": [\"greeting\", \"introduction\", \"meeting start\"]\n  }}\n]\n\nRespond with ONLY the JSON array, no other text.",
      "few_shot_examples": [
        {
          "input": "Small 3-person conversation about project planning",
          "expected_nodes": 5,
          "expected_zoom_distribution": "Mostly levels 3-5, few at 1-2"
        }
      ]
    },
    "detect_contextual_relationships": {
      "description": "Identify contextual/thematic relationships between nodes",
      "model": "gpt-4",
      "temperature": 0.3,
      "max_tokens": 2000,
      "template": "You are analyzing relationships between conversation topics.\n\nGiven these nodes from a conversation:\n\n{nodes_json}\n\nTask: Identify meaningful contextual relationships between nodes. These are NON-sequential connections based on:\n- Shared themes or topics\n- Related concepts or ideas\n- Cause-and-effect relationships\n- Questions and answers across different parts of the conversation\n- References or callbacks to earlier topics\n\nFor each relationship, provide:\n- source_node_id: ID of the source node\n- target_node_id: ID of the target node\n- relationship_type: One of [\"theme\", \"reference\", \"cause_effect\", \"elaboration\", \"contrast\", \"question_answer\"]\n- strength: Float 0.0-1.0 (how strong is this connection)\n- description: Brief explanation of the relationship\n\nReturn a JSON array of relationships.\n\nExample:\n[\n  {{\n    \"source_node_id\": \"node_3\",\n    \"target_node_id\": \"node_7\",\n    \"relationship_type\": \"theme\",\n    \"strength\": 0.8,\n    \"description\": \"Both nodes discuss timeline and deadlines\"\n  }}\n]\n\nRespond with ONLY the JSON array.",
      "constraints": {
        "max_relationships_per_node": 5,
        "min_strength": 0.5
      }
    },
    "refine_node_summary": {
      "description": "Generate or refine a node summary given its utterances",
      "model": "gpt-4",
      "temperature": 0.7,
      "max_tokens": 500,
      "template": "Summarize the following conversation segment in 1-3 sentences:\n\n{utterances_text}\n\nProvide:\n1. A clear, concise summary\n2. Main points discussed\n3. Key decisions or outcomes (if any)\n\nKeep the summary conversational and easy to understand.",
      "output_format": "plain_text"
    },
    "extract_keywords": {
      "description": "Extract key terms and concepts from text",
      "model": "gpt-3.5-turbo",
      "temperature": 0.3,
      "max_tokens": 200,
      "template": "Extract 3-5 key terms or concepts from this text:\n\n{text}\n\nReturn ONLY a JSON array of strings, e.g.: [\"keyword1\", \"keyword2\", \"keyword3\"]",
      "output_format": "json_array"
    },
    "identify_speakers_in_segment": {
      "description": "Identify primary and secondary speakers in a segment",
      "model": "gpt-3.5-turbo",
      "temperature": 0.2,
      "max_tokens": 300,
      "template": "Analyze this conversation segment and identify:\n\n{utterances_text}\n\n1. Primary speaker: Who spoke the most or led the discussion\n2. Secondary speakers: Other active participants\n3. Speaker transitions: Notable hand-offs or back-and-forth\n\nReturn JSON:\n{{\n  \"primary_speaker\": \"Name\",\n  \"secondary_speakers\": [\"Name1\", \"Name2\"],\n  \"transitions\": [{{\"from\": \"Name1\", \"to\": \"Name2\", \"type\": \"question_answer\"}}]\n}}",
      "output_format": "json_object"
    },
    "suggest_zoom_levels": {
      "description": "Suggest appropriate zoom levels for a node based on its characteristics",
      "model": "gpt-3.5-turbo",
      "temperature": 0.2,
      "max_tokens": 100,
      "template": "Given a conversation node with these characteristics:\n- Duration: {utterance_count} utterances\n- Importance: {importance}\n- Granularity: {granularity}\n\nSuggest which zoom levels (1-5) this node should be visible at:\n1 = SENTENCE (most granular)\n2 = TURN\n3 = TOPIC\n4 = THEME\n5 = ARC (least granular)\n\nReturn ONLY a JSON array of integers, e.g.: [3, 4, 5]",
      "output_format": "json_array"
    },
    "simulacra_detection": {
      "description": "Detect Simulacra level (1-4) for conversation nodes based on Baudrillard's theory",
      "model": "claude-3-5-sonnet-20241022",
      "temperature": 0.2,
      "max_tokens": 1024,
      "template": "You are analyzing a conversation node to classify its Simulacra level based on Jean Baudrillard's theory of simulation and hyperreality.\n\n**Simulacra Levels:**\n\n**Level 1 - Reflection of Reality**: Direct factual statements and observable events\n- Examples: \"The meeting started at 2 PM\", \"There are 5 people present\", \"The document has 50 pages\"\n- Characteristics: Verifiable, objective, directly observable, concrete\n\n**Level 2 - Perversion of Reality**: Interpretations, opinions, and subjective representations\n- Examples: \"I think this meeting is productive\", \"The document seems comprehensive\", \"Most people agree\"\n- Characteristics: Subjective but still grounded in reality, personal interpretations, opinions based on observation\n\n**Level 3 - Pretense of Reality**: Hypotheticals and speculation that mask uncertainty\n- Examples: \"If we implement this, it will solve all our problems\", \"This is obviously the best approach\", \"Everyone knows this is how it should be\"\n- Characteristics: Presents uncertain claims as facts, hypotheticals as certainties, assumptions as truths\n\n**Level 4 - Pure Simulacrum**: Abstract concepts disconnected from verifiable reality\n- Examples: \"This paradigm shift will revolutionize everything\", \"We need to leverage synergies\", \"Market forces will naturally optimize outcomes\"\n- Characteristics: Buzzwords, abstractions, self-referential concepts, jargon disconnected from concrete meaning\n\n---\n\n**Node to Analyze:**\n\nTitle: $node_name\n\nSummary: $node_summary\n\nKeywords: $keywords\n\n---\n\n**Task**: Classify this node's primary Simulacra level (1-4) based on the dominant mode of communication.\n\nProvide your analysis in JSON format:\n\n{{\n  \"level\": <1-4>,\n  \"confidence\": <0.0-1.0>,\n  \"reasoning\": \"<Explain why this level was chosen, referencing specific content>\",\n  \"examples\": [\"<quote or paraphrase from the node that exemplifies this level>\", \"<another example>\"]\n}}\n\n**Important:**\n- Choose the DOMINANT level if multiple levels are present\n- Be specific in your reasoning\n- Provide 1-3 concrete examples from the node\n- Confidence should reflect how clearly the node fits the chosen level\n- Return ONLY valid JSON, no other text",
      "output_format": "json_object"
    },
    "bias_detection": {
      "description": "Detect cognitive biases and logical fallacies in conversation nodes",
      "model": "claude-3-5-sonnet-20241022",
      "temperature": 0.3,
      "max_tokens": 2048,
      "template": "You are analyzing a conversation node to identify cognitive biases and logical fallacies.\n\n**Bias Categories:**\n\n**Confirmation Biases** - Seeking information that confirms existing beliefs\n- Confirmation Bias: Favoring information that confirms pre-existing beliefs\n- Cherry Picking: Selecting only supporting data while ignoring contradictory evidence\n- Motivated Reasoning: Reasoning to reach a desired conclusion\n- Belief Perseverance: Maintaining beliefs despite contradictory evidence\n\n**Memory Biases** - Distortions in recall\n- Hindsight Bias: \"I knew it all along\" - seeing past events as predictable\n- Availability Heuristic: Overestimating likelihood based on memorability\n- Recency Bias: Overweighting recent events\n- False Memory: Remembering events incorrectly\n\n**Social Biases** - Group influence\n- Groupthink: Conformity leading to poor decisions\n- Authority Bias: Overvaluing authority opinions\n- Bandwagon Effect: Following the crowd\n- Halo Effect: Positive traits influencing overall judgment\n- In-Group Bias: Favoring one's own group\n\n**Decision-Making Biases** - Systematic judgment errors\n- Anchoring: Over-relying on first information\n- Sunk Cost Fallacy: Continuing based on past investment\n- Status Quo Bias: Preferring current state\n- Optimism Bias: Overestimating positive outcomes\n- Planning Fallacy: Underestimating time/costs\n\n**Attribution Biases** - Explaining behavior\n- Fundamental Attribution Error: Overemphasizing personality, underemphasizing situation\n- Self-Serving Bias: Success = me, failure = external\n- Just World Hypothesis: Believing people get what they deserve\n\n**Logical Fallacies** - Reasoning errors\n- Slippery Slope: One action leads to chain of negative consequences\n- Straw Man: Misrepresenting arguments\n- False Dichotomy: Only two options when more exist\n- Ad Hominem: Attacking person not argument\n- Appeal to Emotion: Manipulating emotions vs reasoning\n- Hasty Generalization: Broad conclusions from limited evidence\n\n---\n\n**Node to Analyze:**\n\nTitle: $node_name\n\nSummary: $node_summary\n\nKeywords: $keywords\n\n---\n\n**Task**: Identify ANY cognitive biases or logical fallacies present in this node.\n\nProvide analysis in JSON format:\n\n{{\n  \"biases\": [\n    {{\n      \"bias_type\": \"<bias_name_snake_case>\",\n      \"category\": \"<confirmation|memory|social|decision|attribution|logical>\",\n      \"severity\": <0.0-1.0>,\n      \"confidence\": <0.0-1.0>,\n      \"description\": \"<How this specific bias manifests in this node>\",\n      \"evidence\": [\"<quote or paraphrase>\", \"<another example>\"]\n    }}\n  ]\n}}\n\n**Important:**\n- Return empty array if NO biases detected (many nodes won't have biases)\n- Only include biases you're confident about (confidence > 0.6)\n- Severity: 0.0 = minor/subtle, 1.0 = severe/obvious\n- Provide specific evidence from the node\n- Use snake_case for bias_type (e.g., \"confirmation_bias\", \"sunk_cost_fallacy\")\n- Return ONLY valid JSON, no other text\n\n**Examples:**\n\nNode: \"Everyone knows this approach is best, we've always done it this way\"\nBiases: [bandwagon_effect (everyone), status_quo_bias (always done it)]\n\nNode: \"The meeting discussed Q3 revenue targets\"\nBiases: [] (factual, no bias)\n\nNode: \"Since we already spent $$100K, we should continue despite poor results\"\nBiases: [sunk_cost_fallacy]",
      "output_format": "json_object"
    },
    "frame_detection": {
      "description": "Detect implicit frames, worldviews, and underlying assumptions",
      "model": "claude-3-5-sonnet-20241022",
      "temperature": 0.3,
      "max_tokens": 2048,
      "template": "You are analyzing a conversation node to identify implicit frames - the underlying worldviews, assumptions, and perspectives that shape how participants interpret and discuss topics.\n\n**Frame Categories:**\n\n**Economic Frames** - Assumptions about markets, value, resources\n- Market Fundamentalism: Markets are best way to organize society\n- Socialist Framework: Collective ownership and equitable distribution\n- Growth Imperative: Continuous growth is necessary\n- Scarcity Mindset: Resources are fundamentally limited\n- Abundance Mindset: There is enough for everyone\n- Zero-Sum Thinking: One's gain is another's loss\n\n**Moral/Ethical Frames** - Underlying ethical principles\n- Utilitarian: Maximizing overall good/happiness\n- Deontological: Focus on duties and rules\n- Virtue Ethics: Character and virtues matter most\n- Care Ethics: Relationships and empathy\n- Rights-Based: Individual rights and freedoms\n- Consequentialist: Judging by outcomes only\n\n**Political Frames** - Assumptions about power and governance\n- Progressive: Social progress and reducing inequality\n- Conservative: Tradition, stability, gradual change\n- Libertarian: Individual liberty, minimal government\n- Authoritarian: Strong central authority\n- Egalitarian: Equality and equal treatment\n- Meritocratic: Success based on merit\n\n**Scientific/Epistemological Frames** - How we know and understand\n- Reductionist: Breaking down into components\n- Holistic: Understanding as integrated wholes\n- Empiricist: Observation and evidence\n- Rationalist: Reason and logical deduction\n- Constructivist: Knowledge as socially constructed\n- Deterministic: Events are causally determined\n\n**Cultural Frames** - Identity, community, social relations\n- Individualist: Individual autonomy priority\n- Collectivist: Group harmony priority\n- Hierarchical: Acceptance of ranked structures\n- Egalitarian: Minimizing status differences\n- Universalist: Universal principles for all\n- Particularist: Context and circumstances matter\n\n**Temporal Frames** - Time, change, progress\n- Short-Term Focus: Immediate concerns\n- Long-Term Thinking: Future impacts matter\n- Cyclical View: Time as recurring patterns\n- Linear Progress: Continuous forward progress\n- Status Quo Permanence: Current conditions persist\n- Radical Change: Transformative disruption expected\n\n---\n\n**Node to Analyze:**\n\nTitle: $node_name\n\nSummary: $node_summary\n\nKeywords: $keywords\n\n---\n\n**Task**: Identify implicit frames - the underlying assumptions and worldviews that shape this discussion.\n\nProvide analysis in JSON format:\n\n{{\n  \"frames\": [\n    {{\n      \"frame_type\": \"<frame_name_snake_case>\",\n      \"category\": \"<economic|moral|political|scientific|cultural|temporal>\",\n      \"strength\": <0.0-1.0>,\n      \"confidence\": <0.0-1.0>,\n      \"description\": \"<How this frame manifests in the node>\",\n      \"evidence\": [\"<quote showing the frame>\", \"<another example>\"],\n      \"assumptions\": [\"<underlying assumption 1>\", \"<assumption 2>\"],\n      \"implications\": \"<What this frame implies about worldview>\"\n    }}\n  ]\n}}\n\n**Important:**\n- Return empty array if NO clear frames (some nodes are frame-neutral)\n- Only include frames with confidence > 0.6\n- Strength: how strongly the frame is present (0.0-1.0)\n- Multiple frames per node are common (e.g., both economic AND moral frames)\n- Assumptions: What must be true for this perspective to make sense?\n- Implications: What does this frame reveal about underlying beliefs?\n- Use snake_case (e.g., \"market_fundamentalism\", \"long_term_thinking\")\n- Return ONLY valid JSON\n\n**Examples:**\n\nNode: \"We need to maximize shareholder value and let the market decide\"\nFrames: [market_fundamentalism (economic), consequentialist (moral)]\nAssumptions: [\"Markets are efficient\", \"Shareholder value is primary goal\"]\n\nNode: \"The meeting covered Q3 logistics\"\nFrames: [] (descriptive, no strong frame)\n\nNode: \"We must consider the long-term environmental impact on future generations\"\nFrames: [long_term_thinking (temporal), care_ethics (moral)]\nAssumptions: [\"Future matters as much as present\", \"We have obligations to future people\"]",
      "output_format": "json_object"
    },
    "detect_claims_three_layer": {
      "description": "Detect factual, normative, and worldview claims in conversation segments",
      "model": "claude-3-5-sonnet-20241022",
      "temperature": 0.3,
      "max_tokens": 4000,
      "template": "You are analyzing a conversation segment to identify three types of claims:\n\n**1. FACTUAL CLAIMS**: Verifiable statements about reality\n- Can be checked against evidence\n- Examples: \"GDP grew 3.2% last quarter\", \"The meeting started at 2pm\", \"There are 50 people in the room\"\n- Characteristics: Objective, concrete, verifiable\n\n**2. NORMATIVE CLAIMS**: Value judgments and prescriptions\n- \"Ought\" statements, preferences, evaluations\n- Examples: \"We should prioritize equality\", \"Healthcare is a human right\", \"This approach is better\"\n- Characteristics: Subjective, evaluative, prescriptive\n- Types:\n  - Prescription: \"We should do X\"\n  - Evaluation: \"X is good/bad\"\n  - Obligation: \"We must do X\"\n  - Preference: \"I prefer X\"\n\n**3. WORLDVIEW CLAIMS**: Implicit ideological frames and hidden assumptions\n- Broad statements that reveal underlying worldview\n- Examples: \"Markets naturally optimize outcomes\", \"Progress requires disruption\", \"Humans evolved to adapt\"\n- Characteristics: Contain hidden premises, ideological markers, naturalistic language\n- Often involve:\n  - Naturalistic fallacies (is → ought)\n  - Hidden assumptions about how the world works\n  - Ideological framing words\n\n---\n\n**Conversation Segment:**\n\n{utterances}\n\n---\n\n**Task**: Identify ALL claims of each type in this segment.\n\n**Instructions**:\n1. Read each utterance carefully\n2. Extract specific claims (not entire utterances)\n3. Classify each claim as factual, normative, or worldview\n4. For factual: Note if verifiable\n5. For normative: Identify type and implicit values\n6. For worldview: Unpack hidden premises and ideological markers\n7. Assign strength (0-1): How central is this claim to the speaker's argument?\n8. Assign confidence (0-1): How confident are you in this classification?\n\n**Return JSON**:\n```json\n{\n  \"claims\": [\n    {\n      \"claim_text\": \"The specific claim extracted\",\n      \"claim_type\": \"factual\" | \"normative\" | \"worldview\",\n      \"speaker\": \"Speaker name\",\n      \"utterance_indices\": [0, 1],\n      \"strength\": 0.8,\n      \"confidence\": 0.9,\n      \n      // FOR FACTUAL CLAIMS ONLY:\n      \"is_verifiable\": true,\n      \n      // FOR NORMATIVE CLAIMS ONLY:\n      \"normative_type\": \"prescription\" | \"evaluation\" | \"obligation\" | \"preference\",\n      \"implicit_values\": [\"fairness\", \"efficiency\"],\n      \n      // FOR WORLDVIEW CLAIMS ONLY:\n      \"worldview_category\": \"economic_neoliberal\" | \"naturalistic_fallacy\" | etc,\n      \"hidden_premises\": [\"Markets are efficient\", \"Growth is inherently good\"],\n      \"ideological_markers\": [\"naturally\", \"rising tide lifts all boats\"]\n    }\n  ]\n}\n```\n\n**Important**:\n- Extract ALL claims, even if multiple per utterance\n- Be specific - extract the exact claim, not the full sentence\n- If an utterance has no claims (e.g., \"Hello\"), skip it\n- Hedged statements (\"I think\", \"maybe\") should have lower strength\n- Strength reflects centrality to argument (0.0 = peripheral, 1.0 = core)\n- Confidence reflects certainty in classification (0.0 = unsure, 1.0 = certain)\n- Return ONLY valid JSON, no other text\n\n**Examples**:\n\n**Input**: \"[0] Alice: GDP grew by 3.2% last quarter according to the report.\"\n**Output**:\n```json\n{\n  \"claims\": [\n    {\n      \"claim_text\": \"GDP grew by 3.2% last quarter\",\n      \"claim_type\": \"factual\",\n      \"speaker\": \"Alice\",\n      \"utterance_indices\": [0],\n      \"strength\": 0.9,\n      \"confidence\": 0.95,\n      \"is_verifiable\": true\n    }\n  ]\n}\n```\n\n**Input**: \"[0] Bob: We should prioritize reducing income inequality over pure growth.\"\n**Output**:\n```json\n{\n  \"claims\": [\n    {\n      \"claim_text\": \"We should prioritize reducing income inequality over pure growth\",\n      \"claim_type\": \"normative\",\n      \"speaker\": \"Bob\",\n      \"utterance_indices\": [0],\n      \"strength\": 0.95,\n      \"confidence\": 0.9,\n      \"normative_type\": \"prescription\",\n      \"implicit_values\": [\"fairness\", \"equality\"]\n    }\n  ]\n}\n```\n\n**Input**: \"[0] Charlie: A rising tide lifts all boats. Economic growth naturally benefits everyone.\"\n**Output**:\n```json\n{\n  \"claims\": [\n    {\n      \"claim_text\": \"Economic growth naturally benefits everyone\",\n      \"claim_type\": \"worldview\",\n      \"speaker\": \"Charlie\",\n      \"utterance_indices\": [0],\n      \"strength\": 0.85,\n      \"confidence\": 0.8,\n      \"worldview_category\": \"economic_neoliberal\",\n      \"hidden_premises\": [\n        \"Markets efficiently distribute benefits\",\n        \"Growth is inherently good\",\n        \"Trickle-down economics works\"\n      ],\n      \"ideological_markers\": [\"rising tide lifts all boats\", \"naturally\"]\n    }\n  ]\n}\n```",
      "output_format": "json_object"
    },
    "build_argument_tree": {
      "description": "Build premise → conclusion argument trees from claims",
      "model": "claude-3-5-sonnet-20241022",
      "temperature": 0.3,
      "max_tokens": 4000,
      "template": "You are analyzing a set of claims to identify argument structures: premises → conclusions.\n\n**Your Task**: Build an argument tree showing how premises support conclusions.\n\n**Argument Types**:\n- **Deductive**: Conclusions logically follow from premises (if premises true, conclusion must be true)\n- **Inductive**: Conclusions probably follow from premises (premises make conclusion likely)\n- **Abductive**: Best explanation given the evidence (inference to best explanation)\n\n**What to Identify**:\n1. **Premises**: Claims that provide support or evidence\n2. **Conclusions**: Claims that are supported or derived from premises\n3. **Argument Structure**: How premises connect to conclusions (can be nested)\n4. **Validity**: Is the logical structure sound?\n5. **Soundness**: Are the premises actually true?\n6. **Fallacies**: Any logical errors?\n7. **Circular Dependencies**: Does the argument assume what it's trying to prove?\n\n---\n\n**Claims to Analyze**:\n\n{claims}\n\n---\n\n**Instructions**:\n1. Identify which claims are premises and which are conclusions\n2. Build a tree structure showing the argument flow\n3. Classify the argument type (deductive/inductive/abductive)\n4. Check for validity and soundness\n5. Identify any fallacies or circular reasoning\n6. If NO argument structure exists (just unrelated claims), return `has_argument: false`\n\n**Return JSON**:\n```json\n{\n  \"has_argument\": true,\n  \"argument_type\": \"deductive\" | \"inductive\" | \"abductive\",\n  \"title\": \"Brief title for this argument\",\n  \"summary\": \"One-sentence summary of the argument\",\n  \"root_claim_id\": \"ID of the main conclusion\",\n  \"tree_structure\": {\n    \"claim_id\": \"conclusion_claim_id\",\n    \"is_conclusion\": true,\n    \"premises\": [\n      {\n        \"claim_id\": \"premise_1_id\",\n        \"is_conclusion\": false,\n        \"premises\": []  // Can be nested if a premise is itself supported\n      },\n      {\n        \"claim_id\": \"premise_2_id\",\n        \"is_conclusion\": false,\n        \"premises\": []\n      }\n    ]\n  },\n  \"is_valid\": true,\n  \"is_sound\": false,\n  \"confidence\": 0.85,\n  \"circular_dependencies\": [],  // List of claim IDs if circular\n  \"identified_fallacies\": [\"hasty_generalization\"],\n  \"explanation\": \"Description of the argument structure and any issues\"\n}\n```\n\n**If no argument exists**:\n```json\n{\n  \"has_argument\": false,\n  \"reason\": \"Claims are unrelated/no logical structure\"\n}\n```\n\n**Important**:\n- Need at least 2 claims to form an argument\n- A claim can be both a conclusion (of one argument) and a premise (for another)\n- Circular reasoning: A → B → C → A\n- Valid but unsound: Logical structure is correct, but premises are false\n- Return ONLY valid JSON, no other text\n\n**Examples**:\n\n**Input Claims**:\n[0] ID: claim_1, Text: \"All humans are mortal\"\n[1] ID: claim_2, Text: \"Socrates is a human\"\n[2] ID: claim_3, Text: \"Therefore, Socrates is mortal\"\n\n**Output**:\n```json\n{\n  \"has_argument\": true,\n  \"argument_type\": \"deductive\",\n  \"title\": \"Socrates is mortal\",\n  \"summary\": \"Socrates must be mortal because all humans are mortal and Socrates is human\",\n  \"root_claim_id\": \"claim_3\",\n  \"tree_structure\": {\n    \"claim_id\": \"claim_3\",\n    \"is_conclusion\": true,\n    \"premises\": [\n      {\"claim_id\": \"claim_1\", \"is_conclusion\": false, \"premises\": []},\n      {\"claim_id\": \"claim_2\", \"is_conclusion\": false, \"premises\": []}\n    ]\n  },\n  \"is_valid\": true,\n  \"is_sound\": true,\n  \"confidence\": 0.95,\n  \"circular_dependencies\": [],\n  \"identified_fallacies\": [],\n  \"explanation\": \"Classic deductive syllogism. Valid form (modus ponens) and sound premises.\"\n}\n```\n\n**Input Claims** (Circular):\n[0] ID: claim_1, Text: \"The Bible is true\"\n[1] ID: claim_2, Text: \"The Bible says God exists\"\n[2] ID: claim_3, Text: \"Therefore God exists, which proves the Bible is true\"\n\n**Output**:\n```json\n{\n  \"has_argument\": true,\n  \"argument_type\": \"deductive\",\n  \"title\": \"Circular argument for God's existence\",\n  \"summary\": \"Bible proves God, God proves Bible\",\n  \"root_claim_id\": \"claim_3\",\n  \"tree_structure\": {\n    \"claim_id\": \"claim_3\",\n    \"is_conclusion\": true,\n    \"premises\": [\n      {\"claim_id\": \"claim_1\", \"is_conclusion\": false, \"premises\": []},\n      {\"claim_id\": \"claim_2\", \"is_conclusion\": false, \"premises\": []}\n    ]\n  },\n  \"is_valid\": false,\n  \"is_sound\": false,\n  \"confidence\": 0.9,\n  \"circular_dependencies\": [\"claim_1\", \"claim_3\"],\n  \"identified_fallacies\": [\"circular_reasoning\", \"begging_the_question\"],\n  \"explanation\": \"Circular argument: assumes Bible is true to prove God exists, then uses God's existence to prove Bible is true.\"\n}\n```",
      "output_format": "json_object"
    },
    "detect_is_ought_conflation": {
      "description": "Detect naturalistic fallacies: jumping from 'is' to 'ought' without justification",
      "model": "claude-3-5-sonnet-20241022",
      "temperature": 0.3,
      "max_tokens": 2000,
      "template": "You are analyzing two claims to determine if there is an IS-OUGHT CONFLATION (naturalistic fallacy).\n\n**The Is-Ought Problem** (David Hume):\n- **IS**: Descriptive statements about how things are (facts about reality)\n- **OUGHT**: Prescriptive statements about how things should be (value judgments)\n- **Fallacy**: Deriving \"ought\" from \"is\" without a normative premise\n\n**Common Fallacy Types**:\n\n**1. Naturalistic Fallacy**: \"X is natural, therefore X is good\"\n- Example: \"Humans evolved to be competitive, so competition is morally right\"\n- Pattern: Natural = Good\n\n**2. Appeal to Nature**: \"X occurs in nature, so X is morally acceptable\"\n- Example: \"Lions kill other lions, so violence among humans is acceptable\"\n- Pattern: Natural behavior justifies human behavior\n\n**3. Appeal to Tradition**: \"We've always done X, so we should continue X\"\n- Example: \"This company has used this process for 50 years, so we should keep it\"\n- Pattern: Historical = Right\n\n**4. Appeal to Consequences**: \"X has outcome Y, therefore X is good\"\n- Example: \"The death penalty deters crime, so it's morally justified\"\n- Pattern: Consequence alone justifies action (missing moral premise)\n\n**5. Is-Ought Jump**: General conflation without specific pattern\n- Example: \"Markets are efficient, so we should have free markets\"\n- Pattern: Descriptive claim → Normative conclusion\n\n**NOT Fallacies** (Valid Reasoning):\n- Normative premise + Fact → Normative conclusion\n  - Example: \"All humans deserve dignity\" (normative) + \"Alice is human\" (fact) → \"Alice deserves dignity\" (valid)\n- Hypothetical reasoning about means to accepted ends\n  - Example: \"If we want X (accepted goal), we should do Y (means)\"\n- Purely descriptive claims with no normative conclusion\n\n---\n\n**Claims to Analyze**:\n\n{claims}\n\n---\n\n**Task**: Determine if the normative claim is improperly derived from the descriptive claim.\n\n**Instructions**:\n1. Identify the descriptive claim (IS) and normative claim (OUGHT)\n2. Check if there's a hidden normative premise that would justify the inference\n3. If no normative premise exists, it's a fallacy\n4. Classify the specific fallacy type\n5. Rate the strength (how obvious is the fallacy?)\n6. Rate your confidence\n\n**Return JSON**:\n```json\n{\n  \"is_conflation\": true,\n  \"fallacy_type\": \"naturalistic_fallacy\" | \"appeal_to_nature\" | \"appeal_to_tradition\" | \"appeal_to_consequences\" | \"is_ought_jump\",\n  \"explanation\": \"Clear explanation of why this is a fallacy\",\n  \"strength\": 0.85,\n  \"confidence\": 0.9,\n  \"conflation_text\": \"Full text showing the is-ought jump\",\n  \"missing_premise\": \"What normative premise would be needed to make this valid\"\n}\n```\n\n**If NOT a conflation**:\n```json\n{\n  \"is_conflation\": false,\n  \"reason\": \"Explanation of why this is valid reasoning\",\n  \"confidence\": 0.85\n}\n```\n\n**Important**:\n- Strength: 0.0 = subtle/debatable, 1.0 = obvious/egregious\n- Confidence: Your certainty in the classification\n- Look for hidden normative premises that might justify the inference\n- Hypothetical reasoning (\"if we want X, do Y\") is NOT a fallacy\n- Return ONLY valid JSON, no other text\n\n**Examples**:\n\n**Input**:\nDescriptive: \"Humans evolved to eat meat\"\nNormative: \"Eating meat is morally acceptable\"\n\n**Output**:\n```json\n{\n  \"is_conflation\": true,\n  \"fallacy_type\": \"appeal_to_nature\",\n  \"explanation\": \"Assumes that what is natural (humans evolved as omnivores) is morally good (eating meat is acceptable). This is a classic naturalistic fallacy - evolution describes what is, not what ought to be.\",\n  \"strength\": 0.85,\n  \"confidence\": 0.9,\n  \"conflation_text\": \"Humans evolved to eat meat, therefore eating meat is morally acceptable\",\n  \"missing_premise\": \"What is natural is morally good (normative premise needed)\"\n}\n```\n\n**Input**:\nDescriptive: \"Alice is a human being\"\nNormative: \"Alice deserves to be treated with dignity\"\n\n**Output**:\n```json\n{\n  \"is_conflation\": false,\n  \"reason\": \"This is valid reasoning IF there is an implicit normative premise 'all humans deserve dignity'. The inference from 'Alice is human' to 'Alice deserves dignity' is not an is-ought jump because it relies on a widely-accepted moral principle about human dignity. This is deductive reasoning from a normative premise, not a naturalistic fallacy.\",\n  \"confidence\": 0.8\n}\n```\n\n**Input**:\nDescriptive: \"Markets allocate resources efficiently\"\nNormative: \"We should use free market policies\"\n\n**Output**:\n```json\n{\n  \"is_conflation\": true,\n  \"fallacy_type\": \"is_ought_jump\",\n  \"explanation\": \"Assumes that efficiency (descriptive property) is automatically good/desirable (normative judgment). Missing normative premise about whether efficiency should be the primary goal. Efficiency could serve bad ends.\",\n  \"strength\": 0.75,\n  \"confidence\": 0.85,\n  \"conflation_text\": \"Markets allocate resources efficiently, so we should use free market policies\",\n  \"missing_premise\": \"Efficiency is a primary moral good that should be maximized (normative premise needed)\"\n}\n```",
      "output_format": "json_object"
    }
  },
  "model_pricing": {
    "gpt-4": {
      "input_per_1k": 0.03,
      "output_per_1k": 0.06
    },
    "gpt-3.5-turbo": {
      "input_per_1k": 0.0005,
      "output_per_1k": 0.0015
    }
  },
  "defaults": {
    "default_model": "gpt-4",
    "default_temperature": 0.5,
    "default_max_tokens": 2000,
    "retry_attempts": 3,
    "timeout_seconds": 30
  }
}